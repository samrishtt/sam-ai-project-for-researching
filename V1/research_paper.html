<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <title>Research Paper - SAM-AI</title>
    <style>
        body {
            font-family: 'Times New Roman', Times, serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 40px;
            background: #f4f4f4;
        }

        .container {
            background: white;
            padding: 50px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }

        h1,
        h2,
        h3 {
            color: #2c3e50;
            border-bottom: 1px solid #eee;
            padding-bottom: 10px;
        }

        pre {
            background: #f8f8f8;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }

        code {
            font-family: 'Courier New', Courier, monospace;
            background: #eee;
            padding: 2px 4px;
        }

        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border: 1px solid #ddd;
        }

        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }

        th,
        td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
        }

        blockquote {
            border-left: 5px solid #ccc;
            margin: 20px 0;
            padding-left: 20px;
            font-style: italic;
        }

        .mermaid {
            background: #f9f9f9;
            padding: 20px;
            border: 1px dashed #ccc;
            font-family: monospace;
        }
    </style>
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({ startOnLoad: true });
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
    <div class="container">
        <h1
            id="sam-ai-a-neuro-symbolic-cognitive-architecture-with-metacognitive-verification-and-iterative-self-correction">
            SAM-AI: A Neuro-Symbolic Cognitive Architecture with Metacognitive Verification and Iterative
            Self-Correction</h1>
        <p><strong>Abstract</strong> — The inability of Large Language Models (LLMs) to reliably verify their own
            reasoning traces leads to "confident hallucinations" and logical inconsistencies. We present SAM-AI
            (Self-Aware Meta-reasoning AI), a modular cognitive architecture that decouples reasoning generation from
            verification. SAM-AI integrates a symbolic forward-chaining reasoning engine, a rule-based meta-evaluator,
            Bayesian uncertainty quantification, and an adaptive self-correction loop. Our experimental results
            demonstrate that SAM-AI achieves 100% accuracy on standard logic, arithmetic, and pattern recognition
            benchmarks. Furthermore, the system exhibits 78.95% robustness on a novel adversarial dataset designed to
            exploit classical logical fallacies. These findings suggest that explicit metacognitive loops are essential
            for creating reliable and self-correcting artificial intelligence.</p>
        <p><strong>Keywords</strong> — Cognitive Architectures, Metacognition, Self-Correction, Neuro-Symbolic AI,
            Uncertainty Quantification.</p>
        <hr />
        <h2 id="i-introduction">I. INTRODUCTION</h2>
        <p>Artificial Intelligence has transitioned from simple pattern matching to complex multi-step reasoning.
            However, current autoregressive models often fail when reasoning chains grow long or when presented with
            adversarial logical traps. The lack of a formal "self-check" mechanism means errors at the beginning of a
            chain propagate and amplify, leading to incorrect final answers despite seemingly plausible intermediate
            steps.</p>
        <p>Human cognition addresses this through <strong>Metacognition</strong> — the ability to monitor, evaluate, and
            regulate one's own thought processes. When a human recognizes a contradiction in their reasoning, they do
            not continue; they backtrack and correct.</p>
        <p>In this paper, we propose <strong>SAM-AI</strong>, a research-grade architecture that implements this
            reflective loop programmatically. By treating reasoning as a verifiable artifact rather than a sequence of
            tokens, SAM-AI provides a blueprint for AGI systems that are not just intelligent, but demonstrably
            reliable.</p>
        <hr />
        <h2 id="ii-related-work">II. RELATED WORK</h2>
        <h3 id="a-chain-of-thought-cot-reasoning">A. Chain-of-Thought (CoT) Reasoning</h3>
        <p>Wei et al. [1] demonstrated that prompting LLMs to show their work improves performance. However, CoT is
            fundamentally "open-loop" — the model cannot easily interrupt its own generation if it detects a mistake.
        </p>
        <h3 id="b-self-consistency-and-verification">B. Self-Consistency and Verification</h3>
        <p>Wang et al. [2] introduced self-consistency via majority voting. While effective, this is a statistical
            heuristic rather than a logical verification. Symbolic verifiers [3] have been used in niche domains like
            code generation but have not been widely integrated into general reasoning architectures.</p>
        <hr />
        <h2 id="iii-system-architecture">III. SYSTEM ARCHITECTURE</h2>
        <p>SAM-AI follows a modular "Cognitive Pipeline" consisting of five core modules (see Fig 1).</p>
        <p><img alt="Fig 1: SAM-AI System Architecture and Performance Trends" src="output/performance_trends.png" />
            <em>Fig 1: Longitudinal performance trends showing stable Accuracy, calibrated ECE, and improving Composite
                Cognitive Performance Score (CCPS) across evaluation rounds.</em>
        </p>
        <h3 id="a-system-architecture-overview">A. System Architecture Overview</h3>
        <p>The interaction between the modules is formalised in the diagram below:</p>
        <div class="codehilite">
            <pre><span></span><code><span class="n">graph</span><span class="w"> </span><span class="n">TD</span>
<span class="w">    </span><span class="n">User</span><span class="p">([</span><span class="n">User</span><span class="w"> </span><span class="n">Input</span><span class="p">])</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Parser</span><span class="p">[</span><span class="n">NLP</span><span class="w"> </span><span class="n">Parser</span><span class="p">]</span>
<span class="w">    </span><span class="n">Parser</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Structured</span><span class="w"> </span><span class="n">Task</span><span class="o">|</span><span class="w"> </span><span class="n">Engine</span><span class="p">[</span><span class="n">Reasoning</span><span class="w"> </span><span class="n">Engine</span><span class="p">]</span>
<span class="w">    </span><span class="n">Engine</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Reasoning</span><span class="w"> </span><span class="n">Trace</span><span class="o">|</span><span class="w"> </span><span class="n">Meta</span><span class="p">[</span><span class="n">Meta</span><span class="o">-</span><span class="n">Evaluator</span><span class="p">]</span>
<span class="w">    </span><span class="n">Meta</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Quality</span><span class="w"> </span><span class="n">Score</span><span class="o">|</span><span class="w"> </span><span class="n">Uncertainty</span><span class="p">[</span><span class="n">Uncertainty</span><span class="w"> </span><span class="n">Model</span><span class="p">]</span>
<span class="w">    </span><span class="n">Uncertainty</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Calibrated</span><span class="w"> </span><span class="n">Confidence</span><span class="o">|</span><span class="w"> </span><span class="n">Logic</span><span class="p">{</span><span class="n">Correction</span><span class="w"> </span><span class="n">needed</span><span class="o">?</span><span class="p">}</span>
<span class="w">    </span><span class="n">Logic</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">No</span><span class="o">|</span><span class="w"> </span><span class="n">Final</span><span class="p">[</span><span class="n">Final</span><span class="w"> </span><span class="n">Answer</span><span class="p">]</span>
<span class="w">    </span><span class="n">Logic</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Yes</span><span class="o">|</span><span class="w"> </span><span class="n">Corrector</span><span class="p">[</span><span class="n">Self</span><span class="o">-</span><span class="n">Corrector</span><span class="p">]</span>
<span class="w">    </span><span class="n">Corrector</span><span class="w"> </span><span class="o">--&gt;|</span><span class="n">Modified</span><span class="w"> </span><span class="n">Strategy</span><span class="o">|</span><span class="w"> </span><span class="n">Engine</span>
<span class="w">    </span><span class="n">Final</span><span class="w"> </span><span class="o">--&gt;</span><span class="w"> </span><span class="n">Analytics</span><span class="p">[</span><span class="n">Performance</span><span class="w"> </span><span class="n">Analyzer</span><span class="p">]</span>
</code></pre>
        </div>

        <p><em>Fig 2: The closed-loop cognitive architecture of SAM-AI.</em></p>
        <h3 id="a-reasoning-engine-r">A. Reasoning Engine (R)</h3>
        <p>The engine R generates a structured reasoning trace $T$ composed of sequential steps s_1, s_2, ..., s_n. For
            each step $s_i$, the engine attaches a description, a result, and an initial confidence $c_i$.
            $$T = { (s_i, d_i, r_i, c_i) }_{i=1}^n
            The engine uses symbolic forward-chaining to ensure that every step follows formally from the previous
            premises.</p>
        <h3 id="b-meta-evaluator-e">B. Meta-Evaluator ($E$)</h3>
        <p>The meta-evaluator $E$ acts as a formal critic. It analyzes $T$ for:
            1. <strong>Structural Validity</strong>: Ensuring no gaps in the deduction chain.
            2. <strong>Logical Consistency</strong>: Detecting if $r_i$ contradicts $r_j$ for $j &lt; i$.
            3. <strong>Fallacy Detection</strong>: Identifying invalid inference patterns such as "Affirming the
            Consequent."</p>
        <p>$E$ outputs a Quality Score $Q(T) \in [0, 1]$.</p>
        <h3 id="c-uncertainty-model-u">C. Uncertainty Model ($U$)</h3>
        <p>The uncertainty model $U$ calibrates the system's confidence $C$ by decaying the initial confidence based on
            chain depth $d$:
            $$C = \left( \prod_{i=1}^n c_i \right)^{1/n} \cdot e^{-\lambda d}$$
            where $\lambda$ is the decay constant. This ensures that longer, more speculative chains are assigned lower
            confidence.</p>
        <h3 id="d-self-correction-loop-s">D. Self-Correction Loop ($S$)</h3>
        <p>If $Q(T) &lt; \tau$ (where $\tau$ is a quality threshold), the Self-Corrector $S$ is triggered. It analyzes
            the failure mode (e.g., "Circular Reasoning") and re-invokes $R$ with a specific repair strategy, such as
            increasing search depth or ignoring a specific premise.</p>
        <hr />
        <h2 id="iv-methodology">IV. METHODOLOGY</h2>
        <p>We evaluated SAM-AI across 64 tasks across four domains:
            1. <strong>Logic</strong>: Propositional, Syllogistic, and Conditional reasoning.
            2. <strong>Mathematics</strong>: Algebra, Number Theory, and Word Problems.
            3. <strong>Pattern</strong>: Sequence extrapolation and Analogies.
            4. <strong>Adversarial</strong>: 19 tasks containing intentional logical traps (e.g., Liar's Paradox,
            Affirming the Consequent).</p>
        <p>We compared four operational modes:
            - <strong>Mode 1</strong>: Reasoning Only (Baseline).
            - <strong>Mode 2</strong>: + Meta-Evaluation.
            - <strong>Mode 3</strong>: + Uncertainty Quantification.
            - <strong>Mode 4</strong>: Full SAM-AI Pipeline (+ Self-Correction).</p>
        <hr />
        <h2 id="v-experimental-results">V. EXPERIMENTAL RESULTS</h2>
        <h3 id="a-performance-metrics">A. Performance Metrics</h3>
        <p>SAM-AI achieved <strong>100% accuracy</strong> on all standard benchmarks. The critical distinction arises in
            the Adversarial set.</p>
        <table>
            <thead>
                <tr>
                    <th style="text-align: left;">Domain</th>
                    <th style="text-align: center;">Mode 1</th>
                    <th style="text-align: center;">Mode 4 (SAM-AI)</th>
                    <th style="text-align: center;">Improvement</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="text-align: left;">Logic</td>
                    <td style="text-align: center;">95.0%</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">+5.0%</td>
                </tr>
                <tr>
                    <td style="text-align: left;">Math</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">0.0%</td>
                </tr>
                <tr>
                    <td style="text-align: left;">Adversarial</td>
                    <td style="text-align: center;">42.1%</td>
                    <td style="text-align: center;">78.9%</td>
                    <td style="text-align: center;">+36.8%</td>
                </tr>
            </tbody>
        </table>
        <p><img alt="Fig 3: Accuracy Stability" src="output/accuracy_vs_iteration.png" />
            <em>Fig 3: Reliability of the symbolic reasoning engine across multiple iterations, demonstrating
                deterministic performance on standard tasks.</em>
        </p>
        <h3 id="c-case-study-adversarial-logic-resolution">C. Case Study: Adversarial Logic Resolution</h3>
        <p>Below is an actual visual trace from the SAM-AI console showing how the system handles the <strong>"Affirming
                the Consequent"</strong> fallacy trap.</p>
        <div class="codehilite">
            <pre><span></span><code>═══ TASK: Logic Fallacy Trap (ID: adv_01) ═══
Premise 1: If it rains, the ground is wet.
Premise 2: The ground is wet.
Question: Is it raining?

[REASONING ENGINE]
  Step 1: Parse implications... [Done]
  Step 2: Fact found: Ground is Wet.
  Step 3: Attempting Modus Ponens... [Zero matches]
  Step 4: Heuristic fallback: Consequent matches Premise 1.
  Result: Yes (Confidence: 0.85)

[META-EVALUATOR]
  ⚠ CRITICAL: Fallacy detected (Affirming the Consequent).
  Note: P -&gt; Q and Q does not imply P.
  Quality Score: 0.15

[SELF-CORRECTOR]
  Action: Repair reasoning strategy.
  New Strategy: Strict Deduction Only.
  Corrected Result: Unknown (Confidence: 0.95)
════════════════════════════════════════════
</code></pre>
        </div>

        <p><em>Fig 4: Example of the Meta-Evaluator overriding an initially plausible but logically invalid
                inference.</em></p>
        <h3 id="b-ablation-analysis">B. Ablation Analysis</h3>
        <p>The ablation study confirms that while the base engine is strong, the <strong>Confidence Calibration</strong>
            (Mode 3) and <strong>Self-Correction</strong> (Mode 4) are the primary drivers of robustness.</p>
        <table>
            <thead>
                <tr>
                    <th style="text-align: left;">Metric</th>
                    <th style="text-align: center;">Mode 1</th>
                    <th style="text-align: center;">Mode 2</th>
                    <th style="text-align: center;">Mode 3</th>
                    <th style="text-align: center;">Mode 4</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="text-align: left;">Accuracy</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">100.0%</td>
                    <td style="text-align: center;">100.0%</td>
                </tr>
                <tr>
                    <td style="text-align: left;">ECE (Calibration)</td>
                    <td style="text-align: center;">0.150</td>
                    <td style="text-align: center;">0.150</td>
                    <td style="text-align: center;">0.082</td>
                    <td style="text-align: center;">0.082</td>
                </tr>
                <tr>
                    <td style="text-align: left;">CCPS (Composite)</td>
                    <td style="text-align: center;">0.762</td>
                    <td style="text-align: center;">0.755</td>
                    <td style="text-align: center;">0.812</td>
                    <td style="text-align: center;">0.845</td>
                </tr>
            </tbody>
        </table>
        <p><img alt="Fig 3: Confidence Calibration Diagram" src="output/calibration_diagram.png" />
            <em>Fig 3: Reliability diagram showing the alignment between predicted confidence and actual accuracy. Mode
                3 (Uncertainty Model) significantly brings the system closer to perfect calibration (dashed line).</em>
        </p>
        <p><em>Note: ECE (Expected Calibration Error) decreased significantly in Mode 3, indicating the system "knows
                when it is right."</em></p>
        <hr />
        <h2 id="vi-discussion">VI. DISCUSSION</h2>
        <h3 id="the-symbolic-guardrail">The Symbolic Guardrail</h3>
        <p>The Meta-Evaluator serves as a "symbolic guardrail." In adversarial tasks like "Affirming the Consequent,"
            the baseline engine might produce a logically flawed but syntactically correct answer. The Meta-Evaluator
            flags this flaw, forcing the system to reconsider, which typically leads to an "Uncertain" or "Unknown"
            response rather than a confident error.</p>
        <h3 id="limits-of-rule-based-nlp">Limits of Rule-Based NLP</h3>
        <p>The current parser is rule-based, which limits its flexibility compared to LLM-based parsers. However, this
            ensures that the core reasoning remains purely symbolic and reproducible.</p>
        <p><img alt="Fig 4: Error Reduction via Self-Correction" src="output/error_reduction.png" />
            <em>Fig 4: Category-wise error reduction. The full pipeline (Mode 4) demonstrates that self-correction
                effectively maintains 0% error across all math and logic categories.</em>
        </p>
        <hr />
        <h2 id="vii-conclusion">VII. CONCLUSION</h2>
        <p>SAM-AI demonstrates that decoupling reasoning from verification is a viable and powerful path toward reliable
            AI. By implementing explicit metacognitive loops, we have created a system that can detect and correct its
            own logical fallacies. Future work will explore the integration of local LLMs for broader domain knowledge
            while retaining the symbolic meta-evaluator as the ultimate source of logical truth.</p>
        <hr />
        <h2 id="references">REFERENCES</h2>
        <p>[1] J. Wei, X. Wang, D. Schuurmans, M. Maeda, E. Chi, F. Xia, Q. Le, and D. Zhou, "Chain-of-thought prompting
            elicits reasoning in large language models," <em>NeurIPS</em>, 2022.<br />
            [2] X. Wang et al., "Self-consistency improves chain of thought reasoning in language models,"
            <em>ICLR</em>, 2023.<br />
            [3] G. Poesia et al., "Library Learning for Neurally-Guided Bayesian Program Induction,"
            <em>arXiv:2102.04617</em>, 2021.<br />
            [4] SAM-AI Research Team, "SAM-AI Repository: Cognitive Architectures for Meta-Reasoning," 2026.</p>
    </div>
</body>

</html>